\chapter{Related Work}
\label{cha:literature}

In this chapter we will go through a number of existing solutions. 
First we introduce the parameters which are important for us and we are interested in them.
It is important to say that we have only considered free and open source projects. Projects
which need royalty fees to be used or do not allow free usage and their code is not available
publicly have not been considered. 

This decision has a very strong background. To offer a 
sustainable solution for small groups with limited budgets and members it is crucial to avoid
any royalty costs in relation with using products and standards of third parties.
The main threat is using a closed source program which neigther us
nor others do not have any chance to improve it according to our needs. The other case which is
even more critical is implementing standards or protocls which might require paying royalty fees.
Therefore we decide to avoid such products, standards or protocols.

\section{Parameters}
We asses existing solutions against our set of requirements and we leave out other factors. 
It is of crucial importance to notice that we asses these products against 
very requirements of small and agile
scientific groups which often only have access to limited resources.
Moreover we look for an application
level solution and not a sole product.
Here are the extracted parameters according to the discussed requirements:

\begin{itemize}
\item Deployment complexity
\item Data provision methods
\item State preservation
\item Centralization
\item Required user rights
\item Runtime control
\item Applicability
\end{itemize}

%\subsubsection {Deployment complexity}
%\subsubsection{Data provision methods}
%\subsubsection {State preservation}
%\subsubsection {Centralization}
%\subsubsection {Required user rights}
%\subsubsection {Runtime control}
%\subsubsection {Applicability}


\section{Grid Computing Solutions}
Here go through a number of projects which are widely being used.
\subsection{UNICORE}
UNICORE is one of the main providers of European Middleware Inititative(EMI) \cite{EMI}. 
It is an open source software under BSD license\footnote{\url{https://www.unicore.eu/unicore/}}.
It follows a client/server architecture and is implemented in Java programming
language.
In consists of three main layers, user, server and target system layer. 
Jobs will be executed from the client machine and the resulting output files will be downloaded
to the same machine as well. It provides job execution and monitoring on grids.\cite{unicore_wp}

It has a graphical client based on Eclipse IDE. There
is also a command line interface available. GUI provides workflow design and execution means and 
allows users to design complex workflows and combine multiple applications while selecting
the desired amount of RAM and number of CPUs on target resources. It introduces a concept 
called GridBeans that allows users to extend the GUI to take advantage of software available on
the grids and visulaizing output data.

The service layer of UNICORE is composed of Gateway and number of other components. Gateway, as
its name says, is the entry point to a UNICORE site. It is like the middle-man between inside
and outside world in UNICORE. Client makes job specification in Job Service Description Language
 (JSDL)\footnote{\url{http://en.wikipedia.org/wiki/Job_Submission_Description_Language}}.
 This layor exposes resources via Web Services Resource Framework (WSRF) compliant web 
 services\footnote{\url{http://en.wikipedia.org/wiki/Web_Services_Resource_Framework}}
 for file transfer, job submission and management. There are also a wide range of file transfer
 protocols supported for site-to-site and 
 client-to-site\footnote{\url{https://www.unicore.eu/unicore/architecture/service-layer.php}}.

It has been used in supercomputing domain to allow exposing and mangaing of available 
computing resources.\cite{unicore_arch}

\subsubsection {Deployment complexity}
Deployment requires Java Runtime Environment in place, which if not available would require 
further assistance from IT administrators to be installed. It is also intented to be installed
on a cluster site not normal workstations. To access web features a browser plugin should be
installed\footnote{
\url{https://www.unicore.eu/documentation/manuals/unicore/files/client_intro.pdf}}.
The server is composed of multiple components that user 
has to download and install each of them separately.
\subsubsection {Required user rights}
To install the server components admin rights on Windows or root access on Unix-like operating
systems are necessary. Even though running the client does not need any special permissions.
\subsubsection{Data provision methods}
Output files are produced on remote resource (service layer) and can be downloaded to client
machine on demand. Input files should be transfered to the UNICORE storage using the client.
There are also command line tools such as \textit{uftp} to transfer files to remote 
storages\footnote{
\url{https://www.unicore.eu/documentation/manuals/unicore/files/uftpclient/uftpclient-manual.pdf}}.
UNICORE relies on standard file system as storage mechanism. However there are efforts to 
extend it to support Hadoop Distributed File System as a storage mean.\cite{wasim2009}
\subsubsection {State preservation}
The UNICORE service layer contains the state of the application and running jobs. 
Upon connecting to a UNICORE site users can utilize the GUI to explore 
the site and access the running jobs for control and monitoring purposes.
The details of possible operations are covered in UNICORE user manuals.

\subsubsection {Centralization}
UNICORE follows a standard client-server architecture therefore it is centralized. It sits on 
top of a cluster and will let clients to connect to it via defined services. There is no means
of inter-site and inter-unicore information exchange. UNICORE is a grid middleware and
infrastructure and therefore it is not intended for inter-site state distribution.
\subsubsection {Runtime control}
It is possible to use UNICORE's web services from third party applications, however this does 
not give control over the run-time internals of the application, instead it is merely a way 
to write extensions for the program.
\subsubsection {Applicability}
There are a number of considerations that prevent us from taking UNICORE as a solution. However
it is a well stablished and mature product on its own.

UNICORE is supposed to be a site manager software. A group have to install it on a cluster and
then users will be able to access the resources. Only a well-informed and skilled group can 
install and maintain such a product, which contradicts with our initial requirement that is 
aimed toward small groups. Moreover we look for a user space solution which is not the case
about UNICORE.

Then next point is distribution of data and state. Even though the product which is installed
on a site will preserve state of jobs and will alow accessing remote file systems, still it is
not a good fit for our case. We need to have automatic inter-site interoperability, both for
data distribution and state of the system, i.e. running jobs, which is not fulfilled by this
product.

Runtime control over a job execution is another limitation that we face with this solution.
We need to be able to deliberately interfere in every step of execution of a job and define 
arbitrary policies for them. To fulfill this, we more need a framework rather than an application.
Applications such as UNICORE represent computing backends and job schedulers rather than a 
platfrom to build new solutions on top of them.

\subsection{Globus Toolkit}
%\subsection{Taverna}
%\subsection{PETSc}
\subsection{Hadoop}

\section{Distributing Data}
\subsection{Distributed File Systems}
One way to achieve fault tolerant and reliable data storage and access is to use
distributed file systems (DFS). In this case the data will be replicated over a
network of storage servers with different magnitudes based on the underlying file
system. We will discuss a number of free and open source solutions.

\subsubsection{Hadoop Distributed File System (HDFS)}
``The Hadoop Distributed File System (HDFS) is a distributed file system designed to run on
commodity hardware.'' \cite[tp.~3]{HDFSDocuments}

``Hadoop1 provides a distributed file system and a framework 
for the analysis and transformation of very large data sets 
using the MapReduce \cite{DG04} paradigm.''\cite{TheHDFS}

``HDFS stores metadata on a
dedicated server, called the NameNode. Application data are stored on
other servers called DataNodes.''\cite{TheHDFS}


\subparagraph{Deployment Complexity}
src:\url{http://hadoop.apache.org/docs/r0.18.3/quickstart.html}
needs Java 1.5.x ssh and sshd and rsync. Three basic modes are available:
Local, Pseudo-Distributed and Fully Distributed mode. XML configuration,
installation of Local and Pseudo Distributed modes are almost straight
forward, for fully distributed note extra steps are required (official
doc link is dead).


\subparagraph{Fault Tolerance}
``Hardware failure is the norm rather than the exception.''
``Each DataNode sends a Heartbeat message to the NameNode periodically.''
``The DataNodes in HDFS do not rely on data protection mechanisms 
such as RAID to make the data durable. Instead, like GFS, 
the file content is replicated on multiple DataNodes for reliability.''
\cite{TheHDFS}

``HDFS has been designed to be easily portable from one platform to another.''

%\subparagraph{Robustness}
%``The primary objective of HDFS is to store data reliably even in the presence of failures.''

\subparagraph{Accessibility}
\begin{enumerate}
\item FS Shell
\item DFSAdmin
\item Browser
\end{enumerate}

\subparagraph{Applicability}
There is a good document here:
\url{http://hadoop.apache.org/docs/r0.18.0/hdfs_design.pdf}
Hints: HADOOP is for big data and the programming should be different (map/reduce)
 and it does not look suitable for our use cases and requirements. The burden would
 be so high that we will waste a lot of resources. I have to put these in scientific
 words with more logic and references to sizes that we need and more numbers.

Users have to program their applications using Java and Hadoop to 
take advantage of distributed computing features in Hadoop MapReduce
and HDFS. Cites? Hadoop website?
\url{https://infosys.uni-saarland.de/publications/BigDataTutorial.pdf}

\section{Distributing State}
In this section we go through a number of existing methods to distributed an object or in another terms to distribute the state.

\subsection{Distributed Hash Tables (DHT)}
Distributed Hash Tables (DHT), best known for their application in building torrent tracking software,
are distributed key/value storages. DHTs could let us to have a key/value store and distributed it in 
a decentralized way among a network of peers.

\subsubsection{Kademlia}
Kademlia is a p2p DHT algorithm introduced in 2002. We first tried to use it as a distributed key/value store but 
it is not suitable for our case and changes do not propagate only to a few neighbours \cite{KademliaPaper}.

In our case to keep track of the available data on the network of collaborating peers, we tried a DHT implementation 
(I was not aware of the problem in the beginning).

Our tests showed that even though DHT is fault-tolerant and reliable for file distribution,
it is not adequate for our realtime requirement to find our required data. In one test we ran two peers,
one on an Internet host and another one on local host. Here are the client and server codes:

\begin{python}
from twisted.application import service, internet
from twisted.python.log import ILogObserver

import sys, os
sys.path.append(os.path.dirname(__file__))
from kademlia.network import Server
from kademlia import log

application = service.Application("kademlia")
application.setComponent(ILogObserver, 
	log.FileLogObserver(sys.stdout, log.INFO).emit)

if os.path.isfile('cache.pickle'):
    kserver = Server.loadState('cache.pickle')
else:
    kserver = Server()
    kserver.bootstrap([("178.62.215.131", 8468)])
kserver.saveStateRegularly('cache.pickle', 10)

server = internet.UDPServer(8468, kserver.protocol)
server.setServiceParent(application)


# Exposing Kademlia get/set API
from txzmq import ZmqEndpoint, ZmqFactory, ZmqREPConnection,
 ZmqREQConnection

zf = ZmqFactory()
e = ZmqEndpoint("bind", "tcp://127.0.0.1:40001")

s = ZmqREPConnection(zf, e)

def getDone(result, msgId, s):
    print "Key result:", result
    s.reply(msgId, str(result))

def doGetSet(msgId, *args):
    print("Inside doPrint")
    print msgId, args

    if args[0] == "set:":
        kserver.set(args[1], args[2])
        s.reply(msgId, 'OK')
    elif args[0] == "get:":
        print args[1]
        kserver.get(args[1]).addCallback(getDone, msgId, s)
    else:
        s.reply(msgId, "Err")

s.gotMessage = doGetSet
\end{python}

In the above example we have used \textit{twisted} networking library\cite{TwistedMatrix} and one
python implementation\cite{KademliaImpl} of \textit{Kademlia} DHT algorithm\cite{KademliaPaper}. 
This will start a p2p network and will try to bootstrap it with another peer on the give IP address.
Thereafter it will open another endpoint to expose a simple \textit{get/set} method for the rest of
application for communicating with the network.

The next part is a few lines of code to communicate with this network:

\begin{python}
#
# Request-reply client in Python
# Connects REQ socket to tcp://localhost:5559
# Sends "Hello" to server, expects "World" back
#
import zmq

# Prepare our context and sockets
context = zmq.Context()
socket = context.socket(zmq.REQ)
socket.connect("tcp://localhost:40001")

# Set request
socket.send(b"set:", zmq.SNDMORE)
socket.send(b"the key", zmq.SNDMORE)
socket.send(b"the value")
print socket.recv()

# Get request
socket.send(b"get:", zmq.SNDMORE)
socket.send(b"the key")
print socket.recv()

# Invalid get
socket.send(b"get:", zmq.SNDMORE)
socket.send(b"not existing")
print socket.recv()
\end{python}

This simple client will try to connect to the previously opened port and send get/set messages.

Configuring this p2p network is a little tricky. The network should work correctly even if nodes enter
and leave the network. During our tests in development environment we observed some problems with initializing the network,
but while the network was initialized leaving and entering the network had no effect on the results.

Having the number of nodes increased up to 3 the reliability shows up again. When we set a value for a key 
in one node we can not guarantee that getting the value for that key on other nodes will return the updated one.
With a number of tests I can confirm that two nodes which are bootstrapped with the same third node does not
provide the accurate result every time and it is not clear for me why this happens. See figure~\ref{fig:threepeers} on page ~\pageref{fig:threepeers}.

After running more tests, we figured out that the possible source of the above mentioned problems 
was the confusion in using \textit{binary} and \textit{string} in python, so it was an error in our side.

\begin{figure}
\centering
\begin{tikzpicture}
  [scale=.8,auto=left,every node/.style={circle,fill=gray!20}]
  \node (n1) at (5,5) {1};
  \node (n2) at (7,2)  {2};
  \node (n3) at (3,2)  {3};

  \foreach \from/\to in {n1/n2,n1/n3,n2/n3}
    \draw (\from) -- (\to);
\end{tikzpicture}
\caption{A network of three peers}
\label{fig:threepeers}
\end{figure}


\subparagraph{Firewall Problems}
In a test having one process running on a server in Internet and outside of the local network and having two
different processes running on one laptop but on different ports it is observed that the changes (sets) in the
Internet does not replicate to the local processes but the changes from local processes are being replicated to the other process.

\subparagraph{conclusion}
Having a network between local and Internet processes in the above mentioned method is not reliable. 
Repeating the tests with only local processes which are bootstrapping to one of them and running the setter/getter
methods showed that even in this scenario it is not reliable and one can not guarantee that the desired value will be returned.


\subsection{Concoord}
Describe why it is not suitable for us. It allows single object sharing.

\section{Distributed Workflows}
In this section we introduce a number of existing scientific workflow systems.
\subsection{COSMOS}\cite{Gafni30062014}
\subsection{Weaver}\cite{Bui_weaver:integrating}