\chapter{Literature}
\label{cha:literature}

In this chapter we will go through a number of existing solutions and we will 
disscuss their efficiency and deployment complexity. Before that we introduce the
parameters which are important for us. Then we will assess each solution against the
introduced parameter set.

\section{Parameters}
There are many factors that we need to take into account before introducing our parameters. To name a few:

\begin{itemize}
\item Data transfer cost
\item Data reproduction cost
\item Type of operation on data, linear or non-linear
\item Replication
\item Deployment complexity
\item Fault tolerance
\item Portability
\item Performance according to number of distributed nodes and size of files
\end{itemize}

\section{Data Storage Systems}
\label{sec:datastoragesystems}

SAME POINTS SHOULD BE DISCUSSED FOR EACH APPROUACH (FROM OUR POINT OF VIEW) 
e.g. EFFICIENCY, COMPLEXITY, DISTRIBUTED APPLICATION ACCESS, APPLICATION AWARENESS,
POSSIBLE DATA ACCESS SCENARIOS, SCALIBILITY, DATA TRANSFER, FAULT TOLERANCE, 
ACCESS CONTROL 


\section{Distributed File Systems}
One way to achieve fault tolerant and reliable data storage and access is to use
distributed file systems (DFS). In this case the data will be replicated over a
network of storage servers with different magnitutes based on the underlying file
system. We will discuss a number of free and open source solutions.

\subsection{Hadoop Distributed File System (HDFS)}
``The Hadoop Distributed File System (HDFS) is a distributed file system designed to run on
commodity hardware.'' \cite[tp.~3]{HDFSDocuments}

``Hadoop1 provides a distributed filesystem and a framework 
for the analysis and transformation of very large data sets 
using the MapReduce \cite{DG04} paradigm.''\cite{TheHDFS}

``HDFS stores metadata on a
dedicated server, called the NameNode. Application data are stored on
other servers called DataNodes.''\cite{TheHDFS}


\subparagraph{Deployment Complexity}
src:\url{http://hadoop.apache.org/docs/r0.18.3/quickstart.html}
needs Java 1.5.x ssh and sshd and rsync. Three basic modes are available:
Local, Pseudo-Distributed and Fully Distributed mode. XML configuration,
installation of Local and Pseudo Distributed modes are almost straight
forward, for fully distributed note extra steps are required (offical
doc link is dead).
\subparagraph{Efficiency}


\subparagraph{Fault Tolerance}
``Hardware failure is the norm rather than the exception.''
``Each DataNode sends a Heartbeat message to the NameNode periodically.''
``The DataNodes in HDFS do not rely on data protection mechanisms 
such as RAID to make the data durable. Instead, like GFS, 
the file content is replicated on multiple DataNodes for reliability.''
\cite{TheHDFS}
\subparagraph{Portability}


``HDFS has been designed to be easily portable from one platform to another.''
\subparagraph{Robustness}

``The primary objective of HDFS is to store data reliably even in the presence of failures.''
\subparagraph{Accessibility}
\begin{enumerate}
\item FS Shell
\item DFSAdmin
\item Browser
\end{enumerate}

\subparagraph{Applicability}
There is a good document here:
\url{http://hadoop.apache.org/docs/r0.18.0/hdfs_design.pdf}
Hints: HADOOP is for big data and the programming should be different (map/reduce)
 and it does not look suitable for our use cases and requirements. The burden would
 be so high that we will waste a lot of resources. I have to put these in scientific
 words with more logic and references to sizes that we need and more numbers.

Users have to program their applications using Java and Hadoop to 
take advantage of distributed computing features in Hadoop MapReduce
and HDFS. Cites? Hadoop website?
\url{https://infosys.uni-saarland.de/publications/BigDataTutorial.pdf}

\subsection{XTREEMFS}

\section{Distributed Objects}
In this section we go through a number of existing methods to distributed an object or in another terms to distribute the state.

\subsection{Concoord}
Describe why it is not suitable for us. It allows single object sharing.

\subsection{Distributed Hash Tables (DHT)}
DHTs are known to be a distributed key/value storage.

\subsubsection{Kademlia}
Kademlia is a p2p DHT algorithm introduced in 2002.

\section{Distributed Workflows}
In this section we introduce a number of existing scientific workflow systems.