\chapter{Discussions}
\label{cha:discussions}

\section{Possible Issues}
\subsection{High Load}
\subsection{Orphan Operations}
\subsection{Complexity Growth}

\subsection{Large Dataset Transfer}
To transfer large arrays over the network there are a number of considerations. Should the array be stored locally before transfer?
What if the array is so big that it does not fit into the machines memory? And how the array should be transferred?

Currently we assume the result datasets fit into memory, therefore there is only the question of how 
to transfer them over the network. To prevent unnecessary copies, we consider streams to send them to
other peers. In the demo application this is done with streaming sockets. The other peer will be notified and then it will fetch the desired dataset.

We need to develop a mechanism to consider dataset size for transfer. User defined files are normally small and we can safely transfer them but
system datasets are large and for any transfer some sort of control should happen.