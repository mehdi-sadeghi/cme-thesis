\chapter{Problem Analysis}
\label{cha:analysis}

In this chapter we try to analyze our problem in depth and find out different aspects of it. We 
discuss a number of important elements such as possible operation types and we formulate the way
we apply operations on datasets.

There are a number of possible use cases in our problem domain. To demonstrate these cases we assume
we have a number of nodes and datasets, and we need one or more datasets to do certain operations.
In this chapter we explain possible combinations of operations, nodes and datasets.

\section{Operations}
In a final solution there would be many services, some will carry administrative tasks such as
getting a list of currently running jobs, or a list of available datasets. These services do not
change state of the system. It means that even though they could affect the performance of the 
running machine, e.g. with querying the database, they will not make any permanent change to 
data stores or the running instance. We are not interested in these services.

There are a number of other services who carry the business logic of our application. Calling
these services will probably change the state of the running instance and might store persistence
data or create new datasets. Moreover they are often data intensive and will trigger some workflows.
 We are interested in these services and we call them operations. They could be
any scientific operation, however we do not discuss the detail of them. Instead we are interested on
categorizing them based on their characteristics, such as type and number of required datasets.
\subsection{Types}
We divide data intensive operations into two main groups, the linear operations and non-linear ones.
This simply comes from the nature of the operation, whether it could be applied to input datasets
in parallel or serial. We describe this with a simple algebraic notion.
%In every scenario we have an operation which could be linear or non-linear.

\subsubsection{Linear operation}
Being linear means that the operation
could be broken into smaller operations and then run in parallel. Having two datasets we can apply the
operation on each of them separately and then aggregate the results. Here is the algebraic notation
of a linear operation which acts on two datasets:

\[ f(a + b) = f(a) + f(b) \]

Being linear or non-linear only matters when we have to operate on more that one dataset, or we want to
break the input into many parts.

Another subtle point here is that for a linear operation we can simply run the operation on the machine that
contains the required dataset, avoiding any dataset transfer among different locations in case the requested
dataset is not available locally on the machine which has received the command to begin the operation.

\subsubsection{Non-linear operation}
In contrast to linear there are non-linear operations. These type of operations require all the inputs to be processed
at once. It is not possible to apply the same operation on each of the input datasets and aggregate the results at the
end. This means that these operations could not be run in parallel.
Here is the algebraic notion of such operation:

\[ f(a + b) \neq f(a) + f(b) \]

A sample of non-linear operations is comparison, when it is required to compare all elements on two different datasets. 
Such a use case happens in many areas of science and engineering. These operations are also called All-Pairs~\cite{moretti08}.
%This is a common use case with material-science community when applying multi-scale modelling.

In non-linear case the complexity of running operations on multiple remote datasets dramatically increases. When the required
datasets are available on the same machine which starts the operation, there is no problem. However when the datasets are not
available on the same machine or even not on a single remote machine, we have to make at least one data transfer. In this 
case at least one dataset should be moved to the location of the other datasets to make it possible to run the non-linear
operation on one single machine which contains both of required datasets.

\subsection{Data files}
As operations need input and produce output datasets, we have to see how the input data will be provided and where the output
data will be stored. Every input or output needs an explicit address, an endpoint, either local or remote.
Typically users will copy files around and will move them using file transfer tools to a working directory and then will
launch desired script using job schedulers. However they do not pass large data files around and will mention their location
inside the script files, which normally points to some shared network storage. 
The scheduler program in turn will run the script at some point of time in future and then
will look for data files inside the given working directory. Finally any output file will be created in the same directory or a location
explicitly defined in the script.
For each operation we need one or more input datasets which might be available on the same node that wants to run the operation
initially or could reside on other nodes. We describe some characteristics of these data files shortly.

\subsubsection{Input}
One need to pay attention that we do not pass complete and real datasets as input parameters,
instead we use identifiers to find the required dataset. Currently for most users these identifiers are nothing
than the name of the data files inside the working directory or the explicit path of a dataset on a network machine.
It is assumed that the system has knowledge of available
datasets and can find them providing an identifier, in this case file name. 
Another point about data files is that they are normally not mission critical and could be reproduced, 
hence emphasizing state and workflow distribution.
 The last point to mention is that input data is not managed by the system.

\subsubsection{Output}
Operations create output datasets which normally are small in size, therefore we ignore the transfer cost of operation
results in our work. These data files will be normally stored in working directories and are of less importance to us.
In typical environments users check output directory for their result and again they use conventional file transfer
tools to have that data locally or provide it to some visualization tools to be visualized. However all these steps
are manual and no control and value added services could be built on top of them. Users are not able to track their
activities and there is no history left about them (except probably some server logs which are of no use to users), 
no reports and statistics could be generated and no
administrative decisions could be made about topics like usages, user activities and popular datasets.
 This all means that the output data files are not managed.

\subsubsection{Intermediate output}
With some of current tools, users can pipe output of one simulation program to another program.
For example if we have dataset \textit{a} as input such a call would look like \(f(g(a))\). 
We will address this further in our design as a \textit{Mixed Operation}.
With a fast-forward we would say that such operations will be handled asynchronously during a number of parent-child operations,
just like function calls in a programming language or mathematics.

\section{Dataset Identification}
When we ask for an operation and we want to store the result somewhere on the network we have to think about an identifier
for them. 
We need a consistent way of naming datasets. If we ask users to provide result dataset names as identifiers it will break soon, because we
would have duplicate names. The naming should be managed by system, as well as data management and transfer itself. 
Generated unique ids for datasets is a possible solution to handle naming problem. 
However, we have to provide a user friendly way for naming, one idea is to assign tags to datasets. If a user search for
these tags, any dataset which has that tag will appear in the query results.
Another approach is to store a database of datasets and operations. Having such a database lets the system to make a relation
between datasets, operations and possible other desired factors.

Even though we want to avoid duplicates in our network, it does not mean we do not want redundancy and replication for our 
data, but it means that we rely on other solutions, such as distributed file systems with built-in replication, to do this task.

\subsection{Data manipulation}
Normally we do not manipulate existing datasets. Each operation results in a number of new datasets. However if we
opt for a storage mechanism such as Hierarchical Data Format (HDF) we might want to store and retrieve datasets from
a single data file presumably in HDF5 format. But it depends on our further design and it does not change the fact
that each operation produces new datasets and we have to store it.

\section{Scenarios}
According to the operation type and number of input datasets, a number of combinations are possible.
In this section we introduce them as scenarios. We begin with a simple one and we gradually add details 
to it and make new scenarios. The following text describes the scenario and it contains expectations of 
our client about the internals of the system, therefore it goes a little into design of the system. 
However in the next chapter we will explain our final approach, therefore any design related material in
this chapter represents only ideas and expectations.%TODO: remove design related material from this chapter

For the following scenarios we assume these general statements to be true:
\begin{enumerate}
\item The user has neither a prior knowledge where the datasets are stored
\item Nor of how many servers are present on the network
\end{enumerate}

\subsection{Scenario 1 - linear operation with one input set}
\label{sc:sc1}
In this scenario we have a linear operation, e.g. \(Op^A\) on \(Node^A\) which
requires one single dataset such as \( Dataset^1 \) which is available on one of the other peers.

We have a distributed network of collaborating servers, where in this case, we consider two computers. 
Each server has its own storage and maintains a number of datasets on it. These servers collaborate 
together to accomplish issued commands. 
User in this case wants to perform one operation on a dataset that resides only on one of the servers. 

The user connects to one of the servers, which we call a client for now. 
This server is assumed to be part of the network, though it may not have any local data stored on it.
The user interactively (or non-interactively) issues a command on a set of datasets providing some kind of identification.
This command is broadcast by the client to all servers in the network.
All servers receive this command and check whether they have the data locally. 
The server which has the data performs the operation and the others ignore it.
The result of the operation in this case, remains on the same server which the original dataset was on. 

\begin{itemize}
%\item Note: it is assumed that at any instance of time, only one server acts as a client.
\item Note: further we will see in our design that the notion of \textit{client-server} does not describe
our design, since it would be more \textit{peer-to-peer} and we would have temporary \textit{result collectors} only.
\end{itemize}

We assume the user has already queried the available data in the entire network by 
issuing a command like “list datasets” which outputs dataset names and ids.

The following table shows two servers, each has one dataset. The user is connected to S2.\\

\begin{tabular}{ l c r }
\em{Server ID} & \em{ Dataset ID} & \em{ Client} \\
S1 & DS1 & No \\
S2 & DS2 & Yes \\
\end{tabular}\\

Let us assume the data sets are \(10^6\) random numbers and
the operation is to transform the real random numbers to a set of [0 or 1 ] depending on whether the number is even or odd. 
This operation is assumed to be a user defined method that operates on the data set and represents user's intended 
logic\footnote{This scenario is partially from a raw requirement text by my advisor.}.

\begin{itemize}
\item Note: A dataset can be defined as an object that has an id, and a one dimensional array (Python list).
\end{itemize}

The user issues the command like this from a command line interface: 

\begin{itemize}
\item real2bin(DS1) will result in  Broadcast(real2bin(DS1))
\item Note: it is assumed that all functions are already defined on all servers%, since they execute the same environment.
\end{itemize}

The client broadcasts this function to all servers. 
Each server will check if the dataset with this id exists, if so will run the command. 

This means that each server, especially the client, has to know about all data sets existing in all servers.
It does not need to have the actual data, but needs to know about it. 
So that when the user issues the above command,
she would not get a non-existing dataset error from the client, just because the data is not there. 
%Hence we need some interface, or some wrapper function that checks the argument for the data type, 
%or to create some proxy interface from all data to all nodes.

Such operations will be done in two simple steps. 
First step would be publishing the received operation to all of the participating peers.
This step is basically transferring the operation to another peer, but in a distributed manner.
Afterwards, when each peer has received the operation request, 
the peer which contains the desired dataset would run the operation in one step,
because this operation requires only one dataset.

In chapter \ref{cha:proposal} we explain in detail our suggested solution.

\subsection{Scenario 2 - linear operation with two input sets}
\label{sc:sc2}
This is similar to scenario one, except that the operation requires two datasets to operate on. 
In this scenario we need at least three peers involved. We assume the first peer has no
data of our interest therefore it should cooperate with others to accomplish the request. 
Our operation in this case requires two 
different input datasets which are not available on the first peer and we should access them on other peers. 

We assume the data distribution is like the following table:\\

\begin{tabular}{ l c r }
\em{Server ID} & \em{ Dataset ID} & \em{ Client} \\
S0 & --- & Yes \\
S1 & DS1 & No \\
S2 & DS2 & No \\
\end{tabular}\\

In this case user will issue a command to \(S^0\) which involves datasets \(DS^1\) and \(DS^2\) 
which none of them are available on \(S^0\).
In this case we need to transfer the operation to other nodes. 
Since this operation is linear we want to break it so it could run in parallel on \(S^1\) and \(S^2\) on
respective datasets.
And then, when the intermediate outputs are ready, they would reside on \(S^1\) and \(S^2\).
Then we would need to transfer one of them to the other node to calculate the final result.
We have to transfer the smaller dataset to have less transfer cost.
Finally when both intermediate datasets are available on one of \(S^1\) or \(S^2\) we run the final operation there.

\iffalse Sample:
\begin{subequations}
\begin{equation}
  \operatorname{min}_{a,b,c} 
  \frac{1}{2}\mathbf{w}^{T}\mathbf{w} + C \sum_{i=1}^{l}\xi_{i} 
\end{equation}    
\begin{equation}
  y_{i}\left(\mathbf{w}^{T}\phi(x_{i})+b\right)
\end{equation}
\end{subequations}
\fi

The simple algebraic notion for these steps would be like these:
\begin{subequations}
\begin{align}
f_{total} &= f_{S0}(DS^1 + DS^2)\\
&= f_{S0}(f_{S1}(DS^1) + f_{S2}(DS^2))
\end{align}
Then we launch them on other nodes
\begin{align}
{DS}^{1.1} = f_{S1}(DS^1)\\
{DS}^{2.1} = f_{S2}(DS^2) 
\end{align}

We move the smaller intermediate dataset to the other node (we assume \(DS^{2.1}\) to be smaller here).
\begin{equation}
DS^{2.1} \longrightarrow S^1
\end{equation}
And finally we launch the operation on \(S^1\) which now contains both sub-datasets.
\begin{align}
f_{total} = f_{S1}(DS^{1.1}) + f_{S1}(DS^{2.1})
\end{align}
\end{subequations}

The transfer scenario would be different in case of mixed operations or non-linear operations.

%The result of this operation will be stored on one of participating peers based on an strategy.
%For now we consider a random storage algorithm among the peers who produce the result.

\subsection{Scenario 3 - linear operation with 2+ input sets}
\label{sc:sc3}
This scenario is slightly different than scenario two only about the number of input datasets. All the assumptions and 
requirements remain the same. Except that we would have one or more extra machines containing the rest of datasets.
However we consider the worst case here, where every machine contains only one of the required datasets and the initial 
machine has none of them.
In reality there would be cases than all the datasets would be available on the same machine or a number of datasets
would exist in one remote machine. The worst data distribution for this case would be like this:
\\
\begin{tabular}{ l c r }
\em{Server ID} & \em{ Dataset ID} & \em{ Client} \\
S0 & --- & Yes \\
S1 & DS1 & No \\
S2 & DS2 & No \\
S3 & DS3 & No \\
\end{tabular}\\

Theoretically there could be more datasets but it is unlikely and often it does not exceed two inputs.


\subsection{Scenario 4 - non-linear operation with one input set}
With the similar assumptions as before, we have only a different type of operation. With one dataset there is no 
difference between this scenario and first scenario, where the operation is linear. 
In case this dataset could
be broken into smaller parts, we should consider the operation type to prevent any unexpected results.

Non-linear scenarios are subject to further work and are not included in current work.

\subsection{Scenario 5 - non-linear operation with two input sets}
This case is very complicated. We can not solve it like the previous ones, and we need to make extra decisions.
In this kind of operation we would need both datasets at the same time in one machine in order to produce any
results. Therefore it is not possible to distribute this operation on multiple machines like we can do for 
a linear operation with multiple inputs. 

\subsection{Scenario 6 - non-linear operation with 2+ input sets}
This is an extension to the previous scenario. 
If we could find a solution for scenario 5, we would extend it to cover this one as well. 
There is no fundamental difference between this scenario and the last one. Again we consider the same
data distribution as described for scenario 3.

\subsection{Scenario 7 - mixed operations}
We previously discussed a number of scenarios to run operations on one ore more datasets. The solution to 
above mentioned scenarios will be discussed in detail in the next chapter. However these are the simple cases and
do not cover all possible operations that we need. Here we introduce operations which are composed of another
operation types which we call them \textit{sub-operation}.
Here is the main assumption before mixing operations:

\begin{itemize}
\item \textbf{Any sub-operation will produce in a new dataset}
\end{itemize}

This is necessary in order to simplify the problem and allow us to make some reasonable results
for a limited set of cases and let aside other possibilities for further work. This would be enough
for us to demonstrate our main problems and also to build our basic proposal on top of that.

Here is the algebraic notion of such a problem, \textit{f} and \textit{g} are linear functions:
%http://oeis.org/wiki/List_of_LaTeX_mathematical_symbols
%http://en.wikibooks.org/wiki/LaTeX/Advanced_Mathematics
\begin{subequations}
\begin{align*}
f_{total} &= f(a + g(b + c))\\
&= f(a) + g(b + c)\\
&= f(a) + g(b) + g(c)
\end{align*}
\end{subequations}

In the above text we assumed linear operations, however we would have mix of linear and non-linear operations. 
We will discuss this further in next chapter, 
while proposing a solution to solve such operations. However we do not cover non-linear cases in current work.

\section{Decision Making}
The main decision that we need to make at every scenario is whether we should transfer the required data or we
need to delegate the operation to an instance on a node which already has the data. To make a decision we need to
answer a number of questions. First we need to know the location of the data:

\begin{enumerate}
\item What is the operation type?
\item Are required datasets available locally?
\end{enumerate}

These points derive directly from our two main requirements about distributing workflow information and 
eliminating data location. But how these questions serve those purposes?

First of all we need to recognize the type of the operation that we are going to launch. This operation could
resemble any of the discussed scenarios in this chapter. This will make it clear for us whether we can directly
jump into distributing the workflow and launching the task or we need to take further steps into breaking
the operation into smaller units. We will discuss this in more detail in next chapter, while describing our design.

The next import question regarding operations is data availability. If a local machine has received an operation
and the required dataset is available on the same machine then there is no need for any transfer, then we would need
only to distribute the operation information among collaborating peers.

Answers to the above questions will help us to decide the machine we should run the operation there. Running an
operation remotely means that we will not transfer back the requested data from another machine, instead we will launch
the operation on the machine containing the data. This is different from conventional approach of transferring methods
or executables to a remote resource and executing it there. In our case we assume that we have the same service API 
available on all of the participating machine. Therefore we only need to decide on which machine we have to forward
the request. In case of forwarding or delegating a request to other machines we would need to preserve regarding
workflow information on all of the participating machines\footnote{A better approach would be designing a state machine and distributing it}. 
This will be discussed in next chapter as part of our distributed workflow management design.