\chapter{Prototype}
\label{cha:prototype}

\section{Architecture}
There are a number of possible ways to design a system to run distributed data intensive operations. Here are 
three well known approaches:
\begin{description}
\item{Conventional Approach} there is no distributed application in this case. Applications run on desktop machines
and they access data on on network storages such as NFS mounted or other distributed file systems. This is pretty much
the same thing that many users of data intensive scentific applications do today.

\item{Centralized Approach} this approach is we have a central orchestrator which users connect to it directly or
using a client to submit their operatinos. This is similar to the traditional client/servier architecture. In 
many HPC distributed applications such as UNICORE, this would be the software which is installed on the cluster
and could have multiple other machines -as resources- under its control. The emphasise in this model is providing
a managed access to distributed computing resources.

\item{Decentralized Approach} in this approach we eliminate the orchestrator peer and the network of
application instances should collaborate in a decentralized fashion to keep track of data and control flow for each
task. This is the paradigm that we will follow in this chapter as our proposed design. 
\end{description}

\subsection{Collaborative design}
To give a better understanding of our solution one should think of it as a distributed collaborative application. 
Even though one instance of our application has same basic functionalities as multiple peers together
but it has been desined for collaboration and a single instance will only be functionl if all the requested data are
available locally. Nevertheless this makes it possible to use application in standalone mode with no peers which might
come beneficial to some users with only local data.

We have picked a decentralized design, where peers will share the knowledge of running operatinos and existing datasets
with one another. The next stark point is that they will collaborate to accomplish one simple or complicated operatin.
In terms of delegating a simple operatino from a node which does not have the required data - but has received the command
to run such operation - to a node which contains the data.

\subsection{Hexagonal Architecture}
In a traditional approach toward application design we would have three tiers, i.e. client layer (GUI),
business layer (logic) and database layer. These tiers correspond to a one dimentional application architecture,
where there are only two \textit{sides} assumed to exist around application logic, client and database. However
this is not the case when we have a multi-dimentional architecture, where there are multiples input/output channels
around our business logic. In the latter case we have to use a so called hexagonal architecture. \cite{alistair}
In such an architecture applications receive signals from multiple communication means at the same time. This 
signals will trigger the appropriate internal business logic, therefore they can't be layered in one dimention.

\subsection{Actors}
There are two types of actors in our problem domain.
\subparagraph{User}
A user who launches, control and monitor an operation.
\subparagraph{Instance}
Every instance can launch and observe an operation on other instances on other nodes.
\subsection{Messaging}
\subsubsection{Publish/Subscribe}
\subsubsection{Filtering}
\subsection{Coupling}
\subsection{State}

\section{Technology}
We have created a python application using Gevent\footnote{\url{http://www.gevent.org/}},
zeromq\footnote{\url{http://zeromq.org/}} and zerorpc\footnote{\url{http://zerorpc.dotcloud.com/}} 
to be able to service multiple requests in a non-blocking way.

\subsection{Programming Language}
We select Python as the main programming language to implement this project. There are a number of 
justifications to do so. Here are the main ones:

\subparagraph{Multi-Platform} Python is a multi-platform language. It runs on different operating systems
seamlessly, hence easier deployment.
\subparagraph{High Availability} Python along with its rich standard library is available by default on almost all Linux machines. This
is a great advantage for use, because we do not need to take further steps to install a runtime in highly conservative institutes.
\subparagraph{Familiarity} Python is already being used as main scripting languages in many scientific environments. This would be an advantage
for us in further steps when users want to contribute to the project or maintain it.
\subparagraph{Based on C} Python is well known to be very close to C programming languages. Even though Python is slow in arithmatic operations
it is possible to write speed critical parts in C and execute it directly within Python code. However in our current solution we do not have
arithmatic operations.
\subparagraph{Faster Development} Since Python does not need special tools to build and deploy its scripts it is much cheaper and faster
to start, build and test programs.
\subparagraph{Aspect Oriented Support} With Python it is very easy to wrap methods and apply pre-process and post-process conditions to them. We have
used this aspect of the language to create operation ids, delegate service calls, distribute messages and etc before and after service calls.

\subsection{Dependency Management} Using \textit{pip}\footnote{\url{https://pypi.python.org/pypi/pip}}
it is very trivial to manage and install multiple
dependencies of a project. It is capable of installing dependencies from remote git repositories or from 
the Python Package Index (PyPI)\footnote{\url{https://pypi.python.org/pypi}}. Moreover \textit{pip} itself is
a Python package. It gives us huge benefits with abstracting away the complexity of dependency management. It can
bundle a package with compiled dependencies, install from Python wheel\footnote{A built-package format for Python}, uninstall,
upgrade and query available PyPI packages.\footnote{\url{https://pip.pypa.io/en/latest/user_guide.html\#create-an-installation-bundle-with-compiled-dependencies}}

\subsection{Virtual Environment}
As described in the previous section, Python is the chosen programming language for this project. Along with Python, 
comes \textit{virtualenv} package\footnote{virtualenv is a tool to create isolated Python environments}. This is a great way of installing
project dependencies into a single directory (which serves as the virtual root file system) and avoid touching operating system managed files and
directories which normal users do not have access to them. While working inside a \textit{virtaulenv}, all the changes is written to a single directory
and all binary files, downloaded Python packages goes into that directory. Therefore this is the best way to deploy a Python project in user space.

\subsection{ØMQ}
The main library that powers our prototype is called ØMQ or ZeroMQ~\cite{ZeroMQ}.
ZeroMQ is an asynchronous messaging library written in C with 
bindings for many languages including python. This library helps us to easily scale and use 
different programming paradigms such as publish-subscribe, request-replay and push-pull.

\section{Compoenents}
\subsection{Distributed Storages}
Our aim is to distribute the information about available datasets and operations at each node. To achieve this
we let our application to launch a number of communicators and publish information about it is data.
Other nodes in our network have to subscribes on other nodes, ZeroMQ allows us to subscribe
to multiple publishers, therefore each node can subscribe to other nodes. Nodes frequently get
\textbf{news} from other nodes, for example availability of certain datasets on a node, then it
can use publish-subscribe to get extra information on that particular subject.
\subsubsection{Operation Store}
\subsubsection{Dataset Store}
\subsection{Message Handlers}
\subsection{Decorators}
\subsection{Application}

\section{Layers}
\subsection{Network}
\subsubsection{API}
Since this is going to be a network program we need to use a form of Remote Procedure Call (RPC) 
to communicate between nodes. Rather than implementing ourselves we used a library based on zeromq 
called \textit{zerorpc}. Using this library we now expose a set of APIs and let the nodes talk to 
each other based on this API. There are multiple solutions for exposing services which we do not discuss here.
\subsubsection{Publisher}
\subsubsection{Listener}
\subsection{Pre-processing}
\subsubsection{Incoming Messages}
\subsubsection{API Calls}
\subsection{Backend}
\subsubsection{Logic}
\subsubsection{Stores}
\subsubsection{Database}

\section{Initialization}
First of all each application instance establish its own zeromq publisher socket. Then it subscribes
itself to all other nodes which are listed in config file.
\subsection{Local Database}
\subsection{Stores}
\subsection{Network}

\section{Control Flows}
\subsection{API Call Flow}
\subsubsection{Possible Reactions}
\subsection{Incoming Message Flow}
\subsubsection{Possible Reactions}

\section{Queries}

\section{Deployment}
The software is easily installable in a Python Virtual Environment.

\section{Test Results}
To be able to assess the performance of each given solution to the mentioned scenarios we made a demo application
called \textbf{Konsensus} which its code is available on Github. \cite{konsensus}

\subsection{Integration Tests}
Writing integration tests for a distributed application is not as straightforward as writing unit-tests for a normal application. 
Our demo application acts as a server and client at the same time. Moreover we want to launch multiple 
network peers running on one or more machines. Testing scenarios on this network is not possible with
normal mocking approaches, because we need to test the behavior of our solution in a network of collaborating
peers which are not external, rather the core services of the application.

To overcome testing issues we have to launch the desired number of peers separately and then run our tests 
over them. To make this operation faster we changed the application to make it possible to launch any number
of instances on one machine and we automated this process using a number of scripts. %TODO more details

\subsubsection{Mixing Signals in Greenlets}
We use python Greenlets instead of threads. This means that our demo application runs on only one thread. 
This causes a problem when launching multiple apps all together with one script and inside one thread, that
causes the signals for events spread among all greenlets and make trouble. To avoid this we have to run
each server in a separate processes. Running them inside threads won't help as well because the blinker python
library is thread-safe so it moves signals between threads as well as Greenlets.

\subsubsection{Scnenario 2 Errors}
While testing scenario 2 we observed a common error. We this scenario with three different peers as the following table:

\begin{tabular}{ l c r }
\em{Server ID} & \em{ Dataset ID} & \em{ Client} \\
S0 & --- & Yes \\
S1 & DS1 & No \\
S2 & DS2 & No \\
\end{tabular}\\

We also used \textbf{Random Dataset Storage} mechanism, simply to store resulting dataset of one operation on one of the network
peers. The problem is when two peers decide to store the result of one operation on each other a blocking condition happens. Our
approach was opening a temporary port and inform the other peer to fetch the data. Meanwhile this is exactly happening on the other
peer, therefore both block.

\subparagraph{Solution}
To solve the blocking peers problem we used the already running main application API instead of opening temporary PULL/PUSH zeromq 
sockets. This change is working fine and the peers exchanging datasets with no problem.
