\chapter{Introduction}
\label{cha:introduction}

\section{Objectives}
There are two main objectives in these thesis as the title suggests.
\begin{itemize}
\item distributing the workflow of application, i.e. the state
\item minimizing the amount of transferred data during an operation
\end{itemize}

First of all we want to focus on collaboration in a distributed application.
This is how multiple computers will manage to finish an operation collectively in a distributed environment.
We want to find a way to keep the state of the running operation distributed amount all the participants.

Next we want to avoid unnecessary data transfer during an operation as much as possible.
We prefer to have the operation be transferred rather than data. However this is not possible all the time,
hence minimizing and smart transfer are mentioned.

Both of these objectives are tailored toward the context that this work is done. This means that even thought 
there are existing workflow management tools and data transfer solutions 
but those tools does not meet our requirements. This will be discussed in more detail in chapter \ref{cha:requirements}.

%To accomplish these objectives we will discuss our specific distributed workflow management and data transfer methods. 
%We define our requirements regarding the above mentioned topics and we will extract the parameters which we are going
%to assess other solutions with them. Then we will go through the currently available solutions and we will discuss them shortly to see
%whether they are applicable to our problem domain with regard to our requirements.

%This data belongs to operations which might be linear or non-linear
%and might involve other operations as well. Each operation can be initiated in any participating
%peer but the required data is not necessarily available on that computer even though
%the operation result might be delivered back to the same peer.


\section{Terms and definitions}
We will use a number of terms through this report. Here are the meaning for each.
\subparagraph{Node}
Refers to one computer in the network.
\subparagraph{Dataset}
We mean both consumed and produced data of scientific applications .e.g. NumPy arrays or HDF5 datasets.
\subparagraph{Application}
The prototype which has been developed to show case the proposed solution, see \ref{cha:prototype}.
\subparagraph{Instance}
An instance of the application running on a node.
\subparagraph{Peer}
One instance of the network application which is in collaboration with other local or remote instances.
\subparagraph{Operation}
Some functions, carrying logic of our application, which users want to run on datasets.
\subparagraph{Task}
Same as the operation with more emphasize on the output rather than the functionality.
\subparagraph{Service}
Remote procedures provided by the application which could be called remotely.
\subparagraph{System}
The combination of nodes, datasets, application, instances, operations and services as a whole.
\subparagraph{User}
A scientist, researcher or student who uses the system.

\section{Problem Context}
European scientific communities launch many experiments everyday, resulting in huge amounts
of data. Specifically in molecular dynamics and material science fields there are many different
simulation softwares which are being used to accomplish multi-scale modeling tasks. 

These tasks often involve running multiple simulation programs over the existing datasets or the data which is
produced by other simulation softwares. It's common to run multiple programs on existing datasets
during one operation to produce the desired results. The order to run simulation softwares is normally 
defined within scripts written by users. 

Moreover users have to provide the required data manually and copy all required files to a working
directory to submit their job, and they might have to login to different machines to prepare files,
submit the script, monitor the status of the job and finally collect the output files. 
This type of work routine is a common form of workflow management in above mentioned communities.

While simpler and smaller experiments could be handled this way, larger and more complicated experiments
require different solutions. 
Such experiments are the source of many high performance computing (HPC) problems, specially workflow management and data transfer.
%In this thesis we work on these issues.

\subsection{Typical Environment}
While working in an institute, often there are many computers which users connect to them remotely. In a typical
scientific and research environment users have their own windows or Linux machine and meanwhile they can SSH to other
Linux machines on the same network with their user credentials. In such environments it is common to have computer clusters
which users access using SSH. Normally there is a job scheduling software such as Sun Grid Engine (SGE) installed on the clusters
and users have to submit their jobs using the tools provided by corresponding cluster software.
Moreover such job schedulers enforce some policies to job submissions.

There are more common characteristics about these environments which we name a few:
\begin{itemize}
\item Users do not have administrator rights and root access to the machines
\item Using network shared storage is very common
\item Institutes often use LDAP\footnote{\url{https://en.wikipedia.org/wiki/Lightweight_Directory_Access_Protocol}} 
and users can login to any machine with their credentials
\end{itemize}

While running multiple scientific programs, they often need to exchange data back and forth in order to accomplish one operation.
Some operations such as comparison require multiple datasets at the same time which also called All-Pairs problem. \cite{moretti08}. 
Such operations involve more computation and are more complicated to address. Because those required datasets 
might not be available on the same machine, hence should be transferred. 
Having these said, it is cheaper to transfer the operation rather than the data whenever possible. 

\section{Document Overview}
This document is organized into \arabic{chapter_count} chapters. Here is a short overview of them:
\subparagraph{Introduction} The opening chapter.
\subparagraph{Requirements} The problems that we are supposed to answer.
\subparagraph{Literature} Introducing the related work.
\subparagraph{Analysis} Different aspects of the problem will be discussed.
\subparagraph{Proposal} We will explain the proposed solution.
\subparagraph{Prototype} Implementation of the proposal.
\subparagraph{Discussions} About the applicability of the proposed solution.
\subparagraph{Conclusion} Summary and future work.