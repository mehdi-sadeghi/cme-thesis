\chapter{Introduction}
\label{cha:introduction}

We are accomplishing this work in order to facilitate creating data-intensive and computationally intensive 
scientific\footnote{So-called e-Science} applications.
Where users often run different simulation software over some pre-existing data sets.
An example of such data sets could be information of a limited number of atoms representing a piece of metal. 
This information would be stored in numeric structures and arrays.
Then there would be applications which will simulate pressure, collision, etc over that data set.

Nowadays this is a common practice in many fields of science, 
where typical non-expert users have to write scripts in order to run their intended applications,
log-in to clusters, finding all required data sets and moving them to a folder
accessible by their script, launch and monitor their scripts and finally collect output files.

Often there would be more than one data set involved in an operation. 
Those data sets would be spread among multiple machines. 
In some cases (such as ours) there would be data being produces on multiple destinations and
they would be required on other machines for some operations.
Then users would have to deal with even more complexity or they would not be able to run
such operations at all or would probably have a poor performance if they do.

Regardless of the subject, this workflow is almost similar. 
Every field will use different simulations but they share same execution process.
In this non-managed approach non-experts have to deal with the complexities of high performance computing systems.

The aforementioned workflow is not flexible and is not managed at all. 
Our goal is to find a solution that let us to mange this situation, 
take the complexities a way from users, create a distributed platform and minimize the transferred data.
Moreover we want to deliver these promises regardless of the workstation that user works with to be
able to run a network of computers which collaborate together to deliver the task. 
This will in turn help us to have more parallelism and avoid single point of failure.

\section{Objectives}
There are two main objectives in these thesis as the title suggests:
\begin{itemize}
\item distributing the workflow of application, i.e. the state
\item minimizing the amount of transferred data during an operation
\end{itemize}

First of all we want to focus on collaboration in a distributed application.
This is how multiple computers will manage to finish an operation collectively in a distributed environment.
We want to find a way to keep the state of the running operation distributed amount all the participants.

Next we want to avoid unnecessary data transfer during an operation as much as possible.
We prefer to have the operation be transferred rather than data. 
However this is not possible all the time,
hence minimizing and smart transfer are mentioned.

Both of these objectives are tailored toward the context that this work is done. 
This means that even thought there are existing workflow management tools and data transfer solutions 
but those tools do not meet our requirements. 
This will be discussed in more detail in chapter \ref{cha:requirements}: Requirements.

%To accomplish these objectives we will discuss our specific distributed workflow management and data transfer methods. 
%We define our requirements regarding the above mentioned topics and we will extract the parameters which we are going
%to assess other solutions with them. Then we will go through the currently available solutions and we will discuss them shortly to see
%whether they are applicable to our problem domain with regard to our requirements.

%This data belongs to operations which might be linear or non-linear
%and might involve other operations as well. Each operation can be initiated in any participating
%peer but the required data is not necessarily available on that computer even though
%the operation result might be delivered back to the same peer.


\section{Terms and definitions}
We will use a number of terms through this report. Here are the meaning for each.
\subparagraph{Node}
Refers to one computer in the network.
\subparagraph{Dataset}
We mean both consumed and produced data of scientific applications .e.g. NumPy arrays or HDF5 datasets.
\subparagraph{Application}
The prototype which has been developed to show case the proposed solution, see \ref{cha:prototype}.
\subparagraph{Instance}
An instance of the application running on a node.
\subparagraph{Peer}
One instance of the network application which is in collaboration with other local or remote instances.
\subparagraph{Operation}
Some functions, carrying logic of our application, which users want to run on datasets.
\subparagraph{Task}
Same as the operation with more emphasize on the output rather than the functionality.
\subparagraph{Service}
Remote procedures provided by the application which could be called remotely.
\subparagraph{System}
The combination of nodes, datasets, application, instances, operations and services as a whole.
\subparagraph{User}
A scientist, researcher or student who uses the system.

\section{Problem Context}
European scientific communities launch many experiments everyday, resulting in huge amounts
of data. Specifically in molecular dynamics and material science fields there are many different
simulation software which are being used to accomplish multi-scale modeling tasks. 

These tasks often involve running multiple simulation programs over the existing datasets or the data which is
produced by other simulation software. It's common to run multiple programs on existing datasets
during one operation to produce the desired results. The order to run simulation software is normally 
defined within scripts written by users. 

Moreover users have to provide the required data manually and copy all required files to a working
directory to submit their job, and they might have to login to different machines to prepare files,
submit the script, monitor the status of the job and finally collect the output files. 
This type of work routine is a common form of workflow management in above mentioned communities.

While simpler and smaller experiments could be handled this way, larger and more complicated experiments
require different solutions. 
Such experiments are the source of many high performance computing (HPC) problems, specially workflow management and data transfer.
%In this thesis we work on these issues.

\subsection{Typical Environment}
While working in an institute, often there are many computers which users connect to them remotely. In a typical
scientific and research environment users have their own windows or Linux machine and meanwhile they can SSH to other
Linux machines on the same network with their user credentials. In such environments it is common to have computer clusters
which users access using SSH. Normally there is a job scheduling software such as Sun Grid Engine (SGE) installed on the clusters
and users have to submit their jobs using the tools provided by corresponding cluster software.
Moreover such job schedulers enforce some policies to job submissions.

There are more common characteristics about these environments which we name a few:
\begin{itemize}
\item Users do not have administrator rights and root access to the machines
\item Using network shared storage is very common
\item Institutes often use LDAP\footnote{\url{https://en.wikipedia.org/wiki/Lightweight_Directory_Access_Protocol}} 
and users can login to any machine with their credentials
\end{itemize}

While running multiple scientific programs, they often need to exchange data back and forth in order to accomplish one operation.
Some operations such as comparison require multiple datasets at the same time which also called All-Pairs problem. \cite{moretti08}. 
Such operations involve more computation and are more complicated to address. Because those required datasets 
might not be available on the same machine, hence should be transferred. 
Having these said, it is cheaper to transfer the operation rather than the data whenever possible. 

In another words the programs that we need to run over existing datasets are distributed among multiple
computers, clusters or HPC sites. The nature of our experiments, makes it necessary to launch
multiple such programs in one run to achieve the desired outcome. This is one reason that we look
for distributed solutions which not only have to distribute state of the program among these machines,
but have to provide smarter ways to move data between these machines during operation executions.

\section{Document Overview}
This document is organized into \arabic{chapter_count} chapters. Here is a short overview of them:
\subparagraph{Introduction} The opening chapter.
\subparagraph{Requirements} The problems that we are supposed to answer.
\subparagraph{Literature} Introducing the related work.
\subparagraph{Analysis} Different aspects of the problem will be discussed.
\subparagraph{Proposal} We will explain the proposed solution.
\subparagraph{Prototype} Implementation of the proposal.
\subparagraph{Discussions} About the applicability of the proposed solution.
\subparagraph{Conclusion} Summary and future work.